{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jddRtEmAIsUP"
   },
   "source": [
    "**Text Feature Extraction : BagofWords Model**\n",
    "\n",
    "Text preprocessing includes feature extraction from the given text data. For this, you can use any of the standard tools such as the bagofwords or k-mer models, as long as they produce sparse output. For this assignment, you should not use models that create dense representations of the text or per-word dense representations (e.g., word2vec).\n",
    "\n",
    "Here is an example of feature extraction using BagOfWords standard tools.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v87CPFMVIkPU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ogl4eM81I5z0"
   },
   "outputs": [],
   "source": [
    "doc = [\"The bag-of-words model is a simplifying representation using natural language processing and information retrieval (IR)\",\n",
    "      \"In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity\",\n",
    "      \"The bag-of-words model has also been used for computer vision.\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QhDwNLJLN9Y"
   },
   "source": [
    "**Sklearn CountVectorizer**\n",
    "\n",
    "token_pattern is defined since CountVectorizer ignores the single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VkcYBK1CLbdB"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "count_occurrences = cv.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1680895517543,
     "user": {
      "displayName": "Sarah Anjum",
      "userId": "17359218465959407143"
     },
     "user_tz": 420
    },
    "id": "A7Kr_h0OLhDp",
    "outputId": "e36c76d1-6fff-47e2-d09c-8439b9ccc778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [3, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_occurrences.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VLzfrSR1Lr-k"
   },
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(data = count_occurrences.toarray(), columns= cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1680895650685,
     "user": {
      "displayName": "Sarah Anjum",
      "userId": "17359218465959407143"
     },
     "user_tz": 420
    },
    "id": "JqvNbOEeL8i6",
    "outputId": "cd99c19b-baf0-456b-d690-57e7ab98a5d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>bag</th>\n",
       "      <th>been</th>\n",
       "      <th>but</th>\n",
       "      <th>computer</th>\n",
       "      <th>disregarding</th>\n",
       "      <th>document</th>\n",
       "      <th>...</th>\n",
       "      <th>simplifying</th>\n",
       "      <th>such</th>\n",
       "      <th>text</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>used</th>\n",
       "      <th>using</th>\n",
       "      <th>vision</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  also  and  as  bag  been  but  computer  disregarding  document  ...  \\\n",
       "0  1     0    1   0    1     0    0         0             0         0  ...   \n",
       "1  3     0    1   2    1     0    1         0             1         1  ...   \n",
       "2  0     1    0   0    1     1    0         1             0         0  ...   \n",
       "\n",
       "   simplifying  such  text  the  this  used  using  vision  word  words  \n",
       "0            1     0     0    1     0     0      1       0     0      1  \n",
       "1            0     1     1    1     1     0      0       0     1      1  \n",
       "2            0     0     0    1     0     1      0       1     0      1  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUh4CbLLjnLO"
   },
   "source": [
    "**Stemming**\n",
    "\n",
    "Stemming is a technique used to reduce an inflected word down to its word stem. Performing this text-processing technique is often useful for dealing with sparsity and/or standardizing vocabulary.\n",
    "\n",
    "Following is a Stemming example in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1680904506680,
     "user": {
      "displayName": "Sarah Anjum",
      "userId": "17359218465959407143"
     },
     "user_tz": 420
    },
    "id": "jEsGVY-gj030",
    "outputId": "6ddfeb4b-a97e-40a0-a8f4-da6f5de2e303"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "Stemming            stem                \n",
      "is                  is                  \n",
      "a                   a                   \n",
      "natural             natur               \n",
      "language            languag             \n",
      "processing          process             \n",
      "technique           techniqu            \n",
      "that                that                \n",
      "lowers              lower               \n",
      "inflection          inflect             \n",
      "in                  in                  \n",
      "words               word                \n",
      "to                  to                  \n",
      "their               their               \n",
      "root                root                \n",
      "forms               form                \n",
      "hence               henc                \n",
      "aiding              aid                 \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "preprocessing       preprocess          \n",
      "of                  of                  \n",
      "text                text                \n",
      "words               word                \n",
      "and                 and                 \n",
      "documents           document            \n",
      "for                 for                 \n",
      "text                text                \n",
      "normalization       normal              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "first_sentence = \"Stemming is a natural language processing technique that lowers inflection in words to their root forms, hence aiding in the preprocessing of text, words, and documents for text normalization.\"\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Remove punctuation\n",
    "first_sentence_no_punct = first_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Create tokens\n",
    "word_tokens = word_tokenize(first_sentence_no_punct)\n",
    "\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in word_tokens:\n",
    "    print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NImmiV9ik_3o"
   },
   "source": [
    "**Lemmatization**\n",
    "\n",
    "Lemmatization is another technique used to reduce inflected words to their root word. It describes the algorithmic process of identifying an inflected word’s “lemma” (dictionary form) based on its intended meaning.\n",
    "\n",
    "In our lemmatization example, we will be using a popular lemmatizer called WordNet lemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1680907239617,
     "user": {
      "displayName": "Sarah Anjum",
      "userId": "17359218465959407143"
     },
     "user_tz": 420
    },
    "id": "tGoTCMfmmOjb",
    "outputId": "664ed656-6810-4342-efd9-6e5000b71c74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/david/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "Lemmatization       Lemmatization       \n",
      "is                  be                  \n",
      "a                   a                   \n",
      "text                text                \n",
      "preprocessing       preprocessing       \n",
      "technique           technique           \n",
      "used                use                 \n",
      "in                  in                  \n",
      "natural             natural             \n",
      "language            language            \n",
      "processing          process             \n",
      "NLP                 NLP                 \n",
      "models              model               \n",
      "to                  to                  \n",
      "break               break               \n",
      "a                   a                   \n",
      "word                word                \n",
      "down                down                \n",
      "to                  to                  \n",
      "its                 its                 \n",
      "root                root                \n",
      "meaning             mean                \n",
      "to                  to                  \n",
      "identify            identify            \n",
      "similarities        similarities        \n",
      "For                 For                 \n",
      "example             example             \n",
      "a                   a                   \n",
      "lemmatization       lemmatization       \n",
      "algorithm           algorithm           \n",
      "would               would               \n",
      "reduce              reduce              \n",
      "the                 the                 \n",
      "word                word                \n",
      "better              better              \n",
      "to                  to                  \n",
      "its                 its                 \n",
      "root                root                \n",
      "word                word                \n",
      "or                  or                  \n",
      "lem                 lem                 \n",
      "me                  me                  \n",
      "good                good                \n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Example sentence\n",
    "second_sentence = \"Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.\"\n",
    "\n",
    "# Remove punctuation\n",
    "second_sentence_no_punc = second_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Create tokens\n",
    "word_tokens = word_tokenize(second_sentence_no_punc)\n",
    "\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\", \"--Lemma--\"))\n",
    "for word in word_tokens:\n",
    "  print(\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz_DkfpVrjcr"
   },
   "source": [
    "Stemming and Lemmatization are both ways to shrink the size of the vocabulary space. Please note that you may choose either of the 2 and not both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klUXCPNJtDOC"
   },
   "source": [
    "**Creating TF-IDF**\n",
    "\n",
    "Following is an example to implement tf-idf technique in python using standard tools, this technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9xy_qHrBr_3j"
   },
   "outputs": [],
   "source": [
    "#calling the TfidfVectorizer\n",
    "vectorize = TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response = vectorize.fit_transform([first_sentence, second_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFD6Nmm4MGBa"
   },
   "source": [
    "Now let's implement the feature extraction module from scratch\n",
    "\n",
    "**Manually (Scratch)**\n",
    "\n",
    "The following function, given a string and a k-mer length parameter c, will create the k-mers for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aRIAN5C-xq9I"
   },
   "outputs": [],
   "source": [
    "def cmer(name, c=3):\n",
    "    r\"\"\" Given a string and parameter c, return the vector of k-mers associated with the words\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    if len(name) < c:\n",
    "        return [name]\n",
    "    v = []\n",
    "    for i in range(len(name)-c+1):\n",
    "        v.append(name[i:(i+c)])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9qjDfZAZMReR"
   },
   "outputs": [],
   "source": [
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "        \n",
    "def textToMatrix(names, c):\n",
    "    docs = [cmer(n, c) for n in names]\n",
    "    return build_matrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1680907272134,
     "user": {
      "displayName": "Sarah Anjum",
      "userId": "17359218465959407143"
     },
     "user_tz": 420
    },
    "id": "Wy5n4o_Kyv45",
    "outputId": "b6ba2429-e571-497a-d76d-6558cd2465e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 3, ncols 28, nnz 74]\n",
      " [nrows 3, ncols 156, nnz 246]\n",
      " [nrows 3, ncols 251, nnz 314]\n"
     ]
    }
   ],
   "source": [
    "csr_info(textToMatrix(doc, 1))\n",
    "csr_info(textToMatrix(doc, 2))\n",
    "csr_info(textToMatrix(doc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBIkJyps6z0s"
   },
   "source": [
    "IMPORTANT: DO NOT MAKE CHANGES TO THIS NOTEBOOK. YOU MAY USE HELP FROM THE GIVEN MODULES BY COPYING THEM TO A NEW NOTEBOOK.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "1. Given the training set and the test set files from the Program 1 assignment, extract features from both using the standard tools and the mannual function (see above).\n",
    "\n",
    "2. Make sure that both document-feature matrices need to be in the same Euclidean space, i.e., the $i$th dimension refers to the same token in both the training and test matrices. Think about different ways you could achieve this with the manual processing function.\n",
    "\n",
    "3. Time processing the datasets using both the standard tools and the mannual method and report the times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI7sbVIH9spW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

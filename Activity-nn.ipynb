{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classification\n",
    "\n",
    "The activity shows an example of designing and training a neural network model using TensorFlow.\n",
    "\n",
    "Reference: https://adventuresinmachinelearning.com/neural-networks-tutorial/, https://www.tensorflow.org/tutorials/quickstart/beginner, https://www.tensorflow.org/tutorials/quickstart/advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  4., 15., 12.,  0.,  0.,  0.,  0.,  3., 16., 15.,\n",
       "       14.,  0.,  0.,  0.,  0.,  8., 13.,  8., 16.,  0.,  0.,  0.,  0.,\n",
       "        1.,  6., 15., 11.,  0.,  0.,  0.,  1.,  8., 13., 15.,  1.,  0.,\n",
       "        0.,  0.,  9., 16., 16.,  5.,  0.,  0.,  0.,  0.,  3., 13., 16.,\n",
       "       16., 11.,  5.,  0.,  0.,  0.,  0.,  3., 11., 16.,  9.,  0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset, show data\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[2]) \n",
    "plt.show()\n",
    "digits.data[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,\n",
       "       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,\n",
       "        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,\n",
       "       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,\n",
       "       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,\n",
       "        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,\n",
       "        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,\n",
       "       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,\n",
       "       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,\n",
       "        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,\n",
       "        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,\n",
       "       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,\n",
       "       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/test\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert output to binary (1-hot encoding) vector\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "y_train[0], y_v_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the neural network\n",
    "nn_structure = [64, 30, 10]\n",
    "def f(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def f_deriv(x):\n",
    "    return f(x) * (1 - f(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm works as follows:\n",
    "\n",
    "Randomly initialise the weights for each layer $W^{(l)}$.  \n",
    "While iterations < iteration limit:  \n",
    "1. Set $\\Delta W$ and $\\Delta b$ to zero.  \n",
    "2. For samples 1 to m:  \n",
    "   a. Perform a feed foward pass through all the $n_l$ layers. Store the activation function outputs $h^{(l)}$.  \n",
    "   b. Calculate the $\\delta^{(n_l)}$ value for the output layer.  \n",
    "   c. Use backpropagation to calculate the $\\delta^{(l)}$ values for layers 2 to $n_l-1$.  \n",
    "   d. Update the $\\Delta W^{(l)}$ and $\\Delta b^{(l)}$  for each layer.  \n",
    "3. Perform a gradient descent step using:  \n",
    "$W^{(l)} = W^{(l)}-\\alpha[\\frac{1}{m}\\Delta W^{(l)}]$,  \n",
    "$b^{(l)} = b^{(l)}-\\alpha[\\frac{1}{m}\\Delta b^{(l)}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters to random values\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))\n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    h = {1: x}\n",
    "    z = {}\n",
    "    for l in range(1, len(W) + 1):\n",
    "        # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "        # it is the output from the last layer\n",
    "        if l == 1:\n",
    "            node_in = x\n",
    "        else:\n",
    "            node_in = h[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "        h[l+1] = f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "    return h, z\n",
    "\n",
    "def calculate_out_layer_delta(y, h_out, z_out):\n",
    "    # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-h_out) * f_deriv(z_out)\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    m = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(len(y)):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored h and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            h, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-h[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[l][:,np.newaxis])) \n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/m * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/m * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/m * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 383 ms, total: 2min 44s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model and plot cost function\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train)\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.17802503477051"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess model performance\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    m = X.shape[0]\n",
    "    y = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        h, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(h[n_layers])\n",
    "    return y\n",
    "\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow beginner version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.72303219e-01,  5.04035710e-01, -2.36671756e-01,\n",
       "        -1.63559060e-01, -2.40954505e-01, -4.37905302e-01,\n",
       "        -2.98322534e-04,  9.52206843e-02,  6.72229779e-02,\n",
       "        -4.81110492e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# load and split dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Stack the layers\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# For each example the model returns a vector of \"logits\" or \"log-odds\" scores, one for each class.\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12247447, 0.17065348, 0.08136357, 0.08753514, 0.08101585,\n",
       "        0.06653273, 0.10305894, 0.11338851, 0.11025792, 0.06371939]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.nn.softmax function converts these logits to \"probabilities\" for each class\n",
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7100613117218018"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The losses.SparseCategoricalCrossentropy loss takes a vector of logits and a True index and returns a scalar loss for each example.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2997 - accuracy: 0.9120\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1450 - accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1086 - accuracy: 0.9675\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0876 - accuracy: 0.9728\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0751 - accuracy: 0.9767\n",
      "CPU times: user 27 s, sys: 3.61 s, total: 30.7 s\n",
      "Wall time: 8.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc930260cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# compile and fit/learn the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.0694 - accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06937223155732469, 0.9788]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load and prepare the MNIST dataset.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Use tf.data to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# Build the tf.keras model using the Keras model subclassing API\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "\n",
    "# Choose an optimizer and loss function for training\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Select metrics to measure the loss and the accuracy of the model.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "# Use tf.GradientTape to train the model\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "# test the model\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1345686917601774, Accuracy: 95.97166666666666, Test Loss: 0.0620757358452936, Test Accuracy: 97.91\n",
      "Epoch 2, Loss: 0.045320465070350716, Accuracy: 98.61333333333333, Test Loss: 0.05382331483114061, Test Accuracy: 98.24000000000001\n",
      "Epoch 3, Loss: 0.024378025516072134, Accuracy: 99.20833333333333, Test Loss: 0.05122035293223484, Test Accuracy: 98.45\n",
      "Epoch 4, Loss: 0.01498234855302629, Accuracy: 99.5, Test Loss: 0.0567034374064936, Test Accuracy: 98.4\n",
      "Epoch 5, Loss: 0.010724055497633648, Accuracy: 99.63, Test Loss: 0.06274510387954933, Test Accuracy: 98.39\n",
      "CPU times: user 26min 3s, sys: 1min 14s, total: 27min 17s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# do the actual training\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
